{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Labs Database Webscraping\n",
      "\n",
      "##Goals\n",
      "* Generate a database of all labs accessible to undergraduates in the Harvard system\n",
      "* Get this data without having to enter it all manually\n",
      "* Learn some cool new webscraping skills that are useful in a bunch of different ways\n",
      "\n",
      "##List of sites to be scraped\n",
      "* Med School: http://hms.harvard.edu/research/basic-and-social-science-departments\n",
      "* SEAS: http://www.seas.harvard.edu/faculty-research/people/name\n",
      "* FAS:\n",
      "    * Astronomy: http://astronomy.fas.harvard.edu/people/filter_by/faculty\n",
      "\t* Chemistry: http://chemistry.harvard.edu/pages/research\n",
      "\t* EPS: http://eps.harvard.edu/people/people/faculty\n",
      "\t* HEB: http://www.heb.fas.harvard.edu/labs.html\n",
      "\t* MCB: https://www.mcb.harvard.edu/mcb/faculty/profile-listing (claimed by Peter as he already did a lot here)\n",
      "\t* Math: http://www.math.harvard.edu/people/senior.html and http://www.math.harvard.edu/people/junior.html\n",
      "\t* OEB: http://www.oeb.harvard.edu/faculty/index.html\n",
      "\t* Physics: https://www.physics.harvard.edu/research/facresearch\n",
      "\t* Statistics: http://statistics.fas.harvard.edu/pages/faculty-research\n",
      "\t* SCRB: http://www.scrb.harvard.edu/research/research-areas\n",
      "* Public Health: http://www.hsph.harvard.edu/researcher-directory/\n",
      "\n",
      "##Process\n",
      "* Install necessary tools (run the block immediately below this one to make sure that you have the necessary software)\n",
      "    * This includes getting added to the Github repo\n",
      "* Find a website that has data on labs / PIs or a list of links to more data\n",
      "* Download all the info from the links / page\n",
      "* Parse the data into a Pandas DataFrame (http://pandas.pydata.org/pandas-docs/stable/10min.html will be your friend)\n",
      "* Save the DataFrame as a csv file\n",
      "* Sync with Github so everyone can get it"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# You only need to run this block once to make sure you have the proper libraries\n",
      "\n",
      "#IPython is what you are using now to run the notebook\n",
      "import IPython\n",
      "print \"IPython version:      %6.6s (need at least 1.0)\" % IPython.__version__\n",
      "\n",
      "# Numpy is a library for working with Arrays\n",
      "import numpy as np\n",
      "print \"Numpy version:        %6.6s (need at least 1.7.1)\" % np.__version__\n",
      "\n",
      "# SciPy implements many different numerical algorithms\n",
      "import scipy as sp\n",
      "print \"SciPy version:        %6.6s (need at least 0.12.0)\" % sp.__version__\n",
      "\n",
      "# Pandas makes working with data tables easier\n",
      "import pandas as pd\n",
      "print \"Pandas version:       %6.6s (need at least 0.11.0)\" % pd.__version__\n",
      "\n",
      "# Module for plotting\n",
      "import matplotlib\n",
      "print \"Mapltolib version:    %6.6s (need at least 1.2.1)\" % matplotlib.__version__\n",
      "\n",
      "# Requests is a library for getting data from the Web\n",
      "import requests\n",
      "print \"requests version:     %6.6s (need at least 1.2.3)\" % requests.__version__\n",
      "\n",
      "#BeautifulSoup is a library to parse HTML and XML documents\n",
      "import BeautifulSoup\n",
      "print \"BeautifulSoup version:%6.6s (need at least 3.2)\" % BeautifulSoup.__version__\n",
      "\n",
      "#Pattern has lots of tools for working with data from the internet\n",
      "import pattern\n",
      "print \"Pattern version:      %6.6s (need at least 2.6)\" % pattern.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "IPython version:       1.0.0 (need at least 1.0)\n",
        "Numpy version:         1.7.1 (need at least 1.7.1)\n",
        "SciPy version:        0.12.0 (need at least 0.12.0)\n",
        "Pandas version:       0.11.0 (need at least 0.11.0)\n",
        "Mapltolib version:     1.2.1 (need at least 1.2.1)\n",
        "requests version:      1.2.3 (need at least 1.2.3)\n",
        "BeautifulSoup version: 3.2.1 (need at least 3.2)\n",
        "Pattern version:         2.6 (need at least 2.6)\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# special IPython command to prepare the notebook for matplotlib\n",
      "%matplotlib inline \n",
      "\n",
      "from fnmatch import fnmatch\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import requests\n",
      "from pattern import web\n",
      "import warnings\n",
      "from random import randint\n",
      "import re\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "\n",
      "# pandas display options\n",
      "pd.set_option('max_rows', 200)\n",
      "pd.set_option('max_columns', 10)\n",
      "\n",
      "# set some nicer defaults for matplotlib\n",
      "from matplotlib import rcParams\n",
      "\n",
      "#these colors come from colorbrewer2.org. Each is an RGB triplet\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843),\n",
      "                (0.4, 0.4, 0.4)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 150\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.grid'] = True\n",
      "rcParams['axes.facecolor'] = '#eeeeee'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'none'\n",
      "\n",
      "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
      "    \"\"\"\n",
      "    Minimize chartjunk by stripping out unnecesasry plot borders and axis ticks\n",
      "    \n",
      "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
      "    \"\"\"\n",
      "    ax = axes or plt.gca()\n",
      "    ax.spines['top'].set_visible(top)\n",
      "    ax.spines['right'].set_visible(right)\n",
      "    ax.spines['left'].set_visible(left)\n",
      "    ax.spines['bottom'].set_visible(bottom)\n",
      "    \n",
      "    # turn off all ticks\n",
      "    ax.yaxis.set_ticks_position('none')\n",
      "    ax.xaxis.set_ticks_position('none')\n",
      "    \n",
      "    # now re-enable visibles\n",
      "    if top:\n",
      "        ax.xaxis.tick_top()\n",
      "    if bottom:\n",
      "        ax.xaxis.tick_bottom()\n",
      "    if left:\n",
      "        ax.yaxis.tick_left()\n",
      "    if right:\n",
      "        ax.yaxis.tick_right()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Synchronize Data\n",
      "\n",
      "Let's load in an example lab (my lab, the Balskus group). This is the kind of data that we would like to get from the sites that we are scraping. Please use the format I suggest here (same column names, etc.). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load the current CSV file into memory\n",
      "labs_df = pd.read_csv('labs_df.csv')\n",
      "\n",
      "# show the first line of the df (labs_df.head() is another option)\n",
      "labs_df.irow(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 61,
       "text": [
        "Unnamed: 0                                                         0\n",
        "PI_email                               balskus@chemistry.harvard.edu\n",
        "PI_name                                            Emily P. Balsksus\n",
        "building                                           Mallinckrodt Labs\n",
        "department                            Chemistry and Chemical Biology\n",
        "funding_sources    ['Searle Foundation', 'Packard Foundation', 'N...\n",
        "lab_desc           The central goal of research in the Balskus gr...\n",
        "lab_location                                               Cambridge\n",
        "lab_name                                                 Balskus Lab\n",
        "lab_url                           http://scholar.harvard.edu/balskus\n",
        "n_members                                                         11\n",
        "pubmed_name                                               Balskus EP\n",
        "tags                   ['Bioorganic', 'Organic', 'Chemical Biology']\n",
        "Name: 0, dtype: object"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##A few helper functions..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def urls_like(url_search, pattern, regex = False):\n",
      "    '''\n",
      "    Returns a list of urls that match a specified pattern from a given page (url_search).\n",
      "    Third parameter determines the pattern matching algorithm: if false, use fnmatch, if true, use regex.\n",
      "    Note: make sure to use entire http:// etc. urls!\n",
      "    '''\n",
      "    \n",
      "    # download data from desired page\n",
      "    html = requests.get(url_search).text.encode('ascii', 'ignore')\n",
      "    \n",
      "    # find all urls that match the given pattern\n",
      "    if regex:\n",
      "        # return the set of all urls that match regex pattern\n",
      "        return list(set(re.findall(pattern, html)))\n",
      "    \n",
      "    else:\n",
      "        # declare dom object to parse the data\n",
      "        dom = web.Element(html)\n",
      "        \n",
      "        # return the set of all unique urls that match fnmatch pattern (* for any char, [0-9] for #, [a-z] for letter, etc.)\n",
      "        return list(set([url.attributes['href'] for url in dom('a') if (fnmatch(url.attributes['href'], pattern))]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Example from CCB"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's look at an example: the EPS department at http://eps.harvard.edu/people/people/faculty\n",
      "# Few things to notice: when you scroll the page, the site calls more stuff so to get everything we need to load\n",
      "# http://eps.harvard.edu/people/people/faculty?page=0 and ?page=1\n",
      "\n",
      "ccb_faculty_list_page = 'http://chemistry.harvard.edu/people/faculty-lecturers/faculty'\n",
      "\n",
      "# we only want pages that look like ..../people/first-(middle-last) and nothing that has another /\n",
      "ccb_pattern = r'http://chemistry.harvard.edu/people/[a-zA-Z]*-[a-zA-Z-%0-9]*'\n",
      "\n",
      "ccb_faculty_pages = list(set(\n",
      "                             urls_like(ccb_faculty_list_page + '?page=0', ccb_pattern, True) +\n",
      "                             urls_like(ccb_faculty_list_page + '?page=1', ccb_pattern, True) +\n",
      "                             urls_like(ccb_faculty_list_page + '?page=2', ccb_pattern, True)))\n",
      "\n",
      "# note that len(ccb_faculty_pages) is a bit long -- there are some pages in here that aren't right: http://chemistry.harvard.edu/people/faculty-lecturers\n",
      "# for example. We will simply ignore their output in the next cell.\n",
      "print \"Number of pages found: \" + str(len(ccb_faculty_pages))\n",
      "print ccb_faculty_pages"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of pages found: 37\n",
        "['http://chemistry.harvard.edu/people/joanna-aizenberg', 'http://chemistry.harvard.edu/people/theodore-betley', 'http://chemistry.harvard.edu/people/david-evans', 'http://chemistry.harvard.edu/people/chemistryfaculty-lecturers', 'http://chemistry.harvard.edu/people/david-r-liu', 'http://chemistry.harvard.edu/people/dan-kahne', 'http://chemistry.harvard.edu/people/erin-oshea', 'http://chemistry.harvard.edu/people/x-sunney-xie', 'http://chemistry.harvard.edu/people/tobias-ritter', 'http://chemistry.harvard.edu/people/matthew-shair', 'http://chemistry.harvard.edu/people/eugene-i-shakhnovich', 'http://chemistry.harvard.edu/people/cynthia-friend', 'http://chemistry.harvard.edu/people/e-j-corey', 'http://chemistry.harvard.edu/people/dudley-herschbach', 'http://chemistry.harvard.edu/people/xiaowei-zhuang', 'http://chemistry.harvard.edu/people/faculty-lecturers', 'http://chemistry.harvard.edu/people/martin-karplus', 'http://chemistry.harvard.edu/people/daniel-g-nocera', 'http://chemistry.harvard.edu/people/hongkun-park', 'http://chemistry.harvard.edu/people/michael-quinn', 'http://chemistry.harvard.edu/people/andrew-myers', 'http://chemistry.harvard.edu/people/emily-balskus', 'http://chemistry.harvard.edu/people/yoshito-kishi', 'http://chemistry.harvard.edu/people/charles-lieber', 'http://chemistry.harvard.edu/people/james-anderson', 'http://chemistry.harvard.edu/people/richard-holm', 'http://chemistry.harvard.edu/people/gregory-verdine', 'http://chemistry.harvard.edu/people/roy-gordon', 'http://chemistry.harvard.edu/people/alan-saghatelian', 'http://chemistry.harvard.edu/people/kang-kuen-ni', 'http://chemistry.harvard.edu/people/william-klemperer', 'http://chemistry.harvard.edu/people/eric-jacobsen', 'http://chemistry.harvard.edu/people/george-whitesides', 'http://chemistry.harvard.edu/people/eric-heller', 'http://chemistry.harvard.edu/people/adam-cohen', 'http://chemistry.harvard.edu/people/jack-szostak', 'http://chemistry.harvard.edu/people/stuart-l-schreiber']\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now we define a function to parse the data from a single drill-down page\n",
      "\n",
      "def cpb_parse(html):\n",
      "    '''\n",
      "    Parse EPS data from given page text and output a DataFrame with the data\n",
      "    '''\n",
      "    try:\n",
      "        # declare a dom parser\n",
      "        dom = web.Element(html)\n",
      "        \n",
      "        # let's build a dict with the necessary info\n",
      "        single = {}\n",
      "        \n",
      "        '''\n",
      "        This is an important trick: as we see in pattern.web, dom[CSS selector] grabs all \n",
      "        elements that match a given selector and returns it as a list that you can parse through.\n",
      "        If you don't know very much about CSS selectors, Firebug (for Firefox) has a great tool that\n",
      "        you can use to right click a given element and select \"print css selector\". It's longer than\n",
      "        necessary but always works.\n",
      "        \n",
      "        If you would prefer using BeautifulSoup instead, that's perfectly fine -- Peter is the expert!\n",
      "        '''\n",
      "    \n",
      "        single['PI_name'] = dom('h1.node-title')[0].content\n",
      "        # more fields here.....\n",
      "        \n",
      "        # return a dataframe containing the info from a single lab\n",
      "        return pd.DataFrame(single, index=['0'])\n",
      "        \n",
      "    # if anything failed, return None\n",
      "    except:\n",
      "        return None\n",
      "\n",
      "html = requests.get('http://chemistry.harvard.edu/people/xiaowei-zhuang').text.encode('ascii', 'ignore')\n",
      "\n",
      "example_labs_df = cpb_parse(html)\n",
      "example_labs_df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>PI_name</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> Xiaowei Zhuang</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 47,
       "text": [
        "          PI_name\n",
        "0  Xiaowei Zhuang"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now we load each of the individual pages and steal everything we can\n",
      "\n",
      "# for every url that we found\n",
      "for url in ccb_faculty_pages:\n",
      "    # try / except blocks attempt to fulfill the code within the try section, if it fails, it runs the except\n",
      "    # let's try to load the given url: if it fails, print it\n",
      "    try:\n",
      "        html = requests.get(url).text.encode('ascii', 'ignore')\n",
      "    except:\n",
      "        print \"Could not find page: \" + str(url)\n",
      "        continue\n",
      "    \n",
      "    # parse a single page\n",
      "    single_lab_df = cpb_parse(html)\n",
      "    \n",
      "    # concat the full df with the single df\n",
      "    if(single_lab_df is not None):\n",
      "        labs_df = pd.concat([single_lab_df, labs_df], ignore_index = True)\n",
      "    else:\n",
      "        print 'Error in ' + url"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save the dataframe as a csv\n",
      "labs_df.to_csv('ccb_labs.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Summary\n",
      "\n",
      "We now have a basic dataframe saved with the labs data from just CCB (well, a very partial one but you get the idea). Next we would clean the data, ensure that it is the same as the pages we scraped, and eventually concat this set of data with the other labs pages.\n",
      "\n",
      "We now need to split everyone up to scrape their own pages -- look forward to an upcoming email for that.\n",
      "\n",
      "Thanks guys!\n",
      "\n",
      "Jonathan"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}